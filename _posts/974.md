title: '《数学之美》 阅读笔记'
tags:
  - 数学之美
  - 算法
id: 974
categories:
  - Learn
  - Reading
  - Note
date: 2013-06-03 15:45:56
---

<div style="text-align: center;"><span style="color: #ff0000; font-size: large;">**文字和语言 vs 数字和信息**</span></div>
<div>不同的文字系统（包括数学）在记录信息上的能力是等价的。</div>
<div>印度人发明了阿拉伯数字，不是阿拉伯人。</div>
<div>如果把中文的笔画作为字母，它其实也是一种拼音文字，不过它是二维的而已。</div>
<div>所以西方的拼音文字称为罗马式的语言。</div>
<div>在东汉以前要将文字刻在其它物件比如龟壳、石碑和竹简上。所以要惜墨如金，这就使得古文异常简洁难懂，而同时期的口语却和今天的白话差别不大（岭南客家话）。</div>
<div>如果说从字母到词的构词法是词的编码规则，那么语法则是语言的编码和解码规则。</div>
<div></div>
<div style="text-align: center;"><span style="color: #ff0000; font-size: large;">**自然语言处理——从规则到统计**</span></div>
<div>计算机能处理自然语言，且处理方法和人类一样。</div>
<div>[**图灵测试**](http://zh.wikipedia.org/zh/%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95)：让人和机器进行交互，如果人无法判断自己交流的对象是人还是机器时，就说明这个机器有智能了。</div>
<div>学习西方语言，都要学习它们的**语法规则，词性，和构词法**等。</div>
<div>自然语言的文法是**上下文有关文法**（Context Dependent Grammar），程序语言是**上下文无关文法**（Context Independent Grammar）。</div>
<div></div>
<div style="text-align: center;"><span style="color: #ff0000; font-size: large;">**统计语言模型**</span></div>
<div>**统计语言模型**（Statistical Language Model）：为自然语言这种上下文相关的特性建立的数学模型。</div>
<div>**二元模型**：对于p(w1,w2,…,wn)=p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)的展开问题，因为p(w3|w1,w2)难计算，p(wn|w1,w2,…,wn-1)更难计算，马尔科夫给出了一个偷懒但是颇为有效的方法，也就是每当遇到这种情况时，就假设任意wi出现的概率只与它前面的wi-1有关，即p(s)=p(w1)p(w2|w1)p(w3|w2)…p(wi|wi-1)…p(wn|wn-1)。现在这个概率就变的简单了。对应的语言模型为二元模型（Bigram Model）。</div>
<div>[**马尔可夫假设**](http://baike.baidu.com/view/1621554.htm)：随机过程中，有一类具有“无后效性质”，即当随机过程在某一时刻to所处的状态已知的条件下，过程在时刻t&gt;to时所处的状态只和to时刻有关，而与to以前的状态无关，则这种随机过程称为马尔科夫过程。 即是：ito为确知,it(t&gt;to)只与ito有关，这种性质为无后效性，又叫马尔科夫假设。![](file:///C:/Users/YEKZ/AppData/Local/youdao/ynote/images/FC21E502F3954C2694FE717D2A6C557F/x1.png)</div>
<div>N-1阶马尔可夫假设对应的语言模型称为N元模型（N-Gram Model）。</div>
<div>Google的**罗塞塔翻译系统**和语言搜索系统，使用的是四元模型。</div>
<div style="text-align: right;">[_<span style="text-decoration: underline;">模型的训练、零概率问题和平滑方法</span>_](http://book.51cto.com/art/201205/337603.htm)</div>
<div>[**大数定律**](https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B)[**（Low of Large Numbers）**](https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B)：只要统计量足够，相对频度就等于概率。</div>
<div>[**古德-图灵估计（Good-Turing Estimage）**](https://zh.wikipedia.org/zh/%E5%9B%BE%E7%81%B5%E4%BC%B0%E8%AE%A1)：Good-Turing平滑法可处理N元语法中数据矩阵的稀疏问题，主要思想将非零N元语法的概率匀给一些低概率语法，以修改最大似然估计与真实概率之间的偏离。</div>
<div>[**Zipf定律（Zipf's Law）**](http://baike.baidu.com/view/1519158.htm)：如果把单词出现的频率按由大到小的顺序排列,则每个单词出现的频率与它的名次的常数次幂存在简单的反比关系。（符合[**二八定律**](https://zh.wikipedia.org/wiki/%E4%BA%8C%E5%85%AB%E5%AE%9A%E5%BE%8B)）。</div>
<div>[**卡茨退避法（Katz backoff**](http://en.wikipedia.org/wiki/Katz)[**）**](http://en.wikipedia.org/wiki/Katz)：对于频率超过一定阈值的词，它们的概率估计就是它们在语料库中的相对频度，对于频率小于这个阈值的词，它们的概率估计就小于他们的相对频度，出现次数越少，频率下调越多。对于未看见的词，也给予一个比较小的概率（即下调得到的频率总和），这样所有词的概率估计都平滑了。</div>
<div><span style="color: #ff0000;"> </span></div>
<div style="text-align: center;"><span style="color: #ff0000; font-size: large;">**隐含马尔可夫模型**</span></div>
<div>雅格布森**通信的六要素**：发送者（信息源），信道，接收者，信息，上下文和编码。</div>
<div>符合马尔科夫假设（各个状态st的概率分布只与它前一个状态st-1有关）的随机过程成为马尔可夫过程，也称为马尔可夫链。</div>
<div>[**隐含马尔科夫模型**](https://zh.wikipedia.org/wiki/%E9%9A%90%E5%90%AB%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B)是马尔科夫链的扩展，任意时刻t的状态st是不可见的，所以观察者没法通过观察到一个状态序列s1,s2,s3,…,sT来推测转移概率等参数。但是隐马尔科夫模型在每个时刻t会输出一个符号ot，而且ot和st相关且仅和ot相关。这个被称为独立输出假设。其中隐含的状态s1,s2,s3,…是一个典型的马尔科夫链。</div>
<div></div>
<div>延伸阅读：隐含马尔可夫模型的训练</div>
<div>隐含马尔科夫模型的三个基本问题：</div>
<div>

1.  给定一个模型，如何计算某个特定的输出序列的概率。（[**Forward-Backward算法**](http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm)）
2.  给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列。（[**维特比算法**](http://zh.wikipedia.org/zh/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95)）
3.  给定足够量的观测数据，如何估计隐含马尔可夫模型的参数。
<div>**转移概率**：从前一个状态St-1进入当前状态St的概率P(St|St-1)。</div>
</div>
<div>**生成概率**：每个状态St产生相应输出符号Ot的概率P(Ot|St)。</div>
<div>转移概率和生成概率被称为隐含马尔可夫模型的参数，而计算或者估计这些参数的过程称为**模型的训练**。</div>
<div>有监督的训练方法：人工标注数据。</div>
<div>无监督的训练方法：**鲍姆-韦尔奇算法（Baum-Welch Algorithm）**：首先找到一组能够产生输出序列O的模型参数，这个初始模型成为Mtheta0，需要在此基础上找到一个更好的模型，假定不但可以算出这个模型产生O的概率P(O|Mtheta0)，而且能够找到这个模型产生O的所有可能的路径以及这些路径的概率。并算出一组新的模型参数theta1，从Mtheta0到Mtheta1的过程称为一次迭代。接下来从Mtheta1出发寻找更好的模型Mtheta2，并一直找下去，直到模型的质量没有明显提高为止。这样一直估计（Expectation）新的模型参数，使得输出的概率达到最大化（Maximization）的过程被称为期望值最大化（Expectation-Maximization）简称**EM过程**。EM过程能保证一定能收敛到一个局部最优点，但不能保证找到全局最优点。因此，在一些自然语言处理的应用中，这种无监督的鲍姆-韦尔奇算法训练处的模型比有监督的训练得到的模型效果略差。</div>
<div>隐含马尔科夫模型是机器学习主要工具之一，和几乎所有机器学习的模型工具一样，它需要一个训练算法（鲍姆-韦尔奇算法）和使用时的解码算法（维特比算法）。掌握了这两类算法，就基本上可以使用隐含马尔科夫模型这个工具了。</div>
<div style="text-align: right;">_[隐马尔可夫模型攻略](http://www.leexiang.com/hidden-markov-model)_</div>
<div></div>
<div style="text-align: center;"><span style="color: #ff0000; font-size: large;">**信息的度量和作用**</span></div>
<div>[香农三定理](http://baike.baidu.com/view/497143.htm)</div>
<div style="text-align: right;">_[通信的数学原理-香农](http://ishare.iask.sina.com.cn/f/6390611.html "通信的数学原理-香浓.英文.pdf")_</div>
<div></div>
<div>[**信息熵（Entropy）**](http://en.wikipedia.org/wiki/Entropy_(information_theory))：信息量就等于不确定性的多少。</div>
<div>计算公式：H(x)=E[I(xi)]=E[ log(2,1/p(xi)) ]=-∑p(xi)log(2,p(xi)) (i=1,2,..n)</div>
<div>[**冗余度（Redundancy）**](http://baike.baidu.com/view/1030522.htm)：数据的重复度。在一个数据集合中重复的数据称为数据冗余。</div>
<div style="text-align: right;">_[汉语信息熵和语言模型的复杂度](http://ishare.iask.sina.com.cn/f/33403276.html "本文介绍了估计汉语信息摘的方法，井通过对大量语料的统计，给出了汉语信息熵的一个上界－５．１７比特／汉字。本文还以此为基础对统计语言模型的能力进行了定量的描述，比较了常用的统计语言模型的性能，并给出了一种用低阶语言模型逼近高阶模型的方法。")_</div>
<div></div>
<div>[**互信息(Mutual Information)**](http://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF)：对两个随机事件“相关性”的量化度量。</div>
<div></div>
<div>[**相对熵（交叉熵，KL散度，<span style="font-family: sans-serif;">Kullback–Leibler divergence，KLD</span>）**](http://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5)：用来度量两个取值为正数的函数的相似性。</div>
<div>

1.  对于两个完全相同的函数，它们的相对熵等于零。
2.  相对熵越大，两个函数差异越大；反之，相对熵越小，两个函数差异越小。
3.  对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性。
<div>**语言模型复杂度（Perplexity）**：在给定上下文的条件上，句子中每个位置平均可以选择的单词数量。复杂度越小，每个位置的词就越确定，模型越好。</div>
</div>
<div></div>
<div style="text-align: center;"><span style="color: #ff0000; font-size: large;">**贾里尼克和现代语言处理**</span></div>
<div>

1.  小学生和中学生没必要花那么多时间读书，他们的社会经验、生活能力以及在那时树立起的志向将帮助他们的一生。
2.  中学阶段花很多时间比同伴多读的课程，在大学以后可以用非常短的时间就可以读完，在大学阶段，人的理解了要强得多。
3.  学习和教育是一个人一辈子的事。
4.  书本的内容可以早学，也可以晚学，但是错过了成长阶段却是无法弥补的。
<div><span style="color: #ff0000;">**Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things.**</span></div>
<div></div>
<div>网络爬虫问题：如何在有限时间里最多地爬下最重要的网页，在这个前提下，显然BFS明显优于DFS。</div>
</div>
<div>**TF-IDF ( Term Frequency / Inverse Document Frequency )**</div>
<div>**TF**：单文本词频</div>
<div>**IDF**：逆文本词频指数，公式：**log(D/Dw)** D：全部网页数 Dw：关键词w在Dw个网页中出现过。</div>
<div>

1.  一个词预测主题的能力越强，权重越大，反之，权重越小。
2.  停止词（是、和、中、地。。。）的权重为零。
<div>模糊匹配问题的解决总是依靠马尔科夫链。</div>
</div>
<div>计算机是如此重要，因此不能只留给男人去做！</div>
<div></div>
<div>**[有限状态机](https://zh.wikipedia.org/zh/%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA)：**表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。</div>
<div>**加权的有限状态传感器（WFST）**：有限状态机中的每个状态由输入和输出符号定义，根据输入和输出可能性的不同赋以权重。WFST中的每一条路径就是一个候选的句子，其中概率最大的那条路径就是句子的识别结果。</div>
<div></div>
<div>先帮助用户解决80%的问题，再慢慢解决剩下的20%问题，是在工业界成功的秘诀之一。</div>
<div></div>
<div>余弦定理进行新闻分类：</div>
<div>

1.  新闻的特征向量：一篇新闻中N个词的TF-IDF值构成的N维向量。向量中每一个维度的大小代表每个词对这篇新闻主题的贡献。
2.  计算所有新闻之间两两的余弦相似性，把相似性大于一个阀值的新闻合并成一个小类。这样N篇新闻就被合并成N1个小类，当然N1&lt;N。
3.  把每个小类中所有的新闻作为一个整体，计算小类的特征向量，再计算小类之间两两的余弦相似性，然后合并成大一点的小类，假如有N2个，当然N2&lt;N1。
</div>
<div>如果对一段信息进行无损压缩编码，理论上编码后的最短长度就是它的信息熵。</div>
<div>**伪随机数产生器算法**：将任意很长的整数转换层特定长度的伪随机数。</div>
<div>**基于加密的伪随机数产生器：MD5、SHA-1**</div>
<div>视频匹配的两个核心技术：关键帧的提取和特征的提取。MPEG视频每一帧之间差异不大，只有极少数的帧是完整的图像，这些称为关键帧。其余帧储存的只是和关键帧相比的差异值。</div>
<div></div>
<div>[**公开密匙加密**](http://zh.wikipedia.org/wiki/%E5%85%AC%E5%BC%80%E5%AF%86%E9%92%A5%E5%8A%A0%E5%AF%86)：也称为非对称密钥，每个人都有一对唯一对应的密钥：公开密钥（简称公钥）和私人密钥（简称私钥），公钥对外公开，私钥由个人秘密保存；用其中一把密钥加密，就只能用另一把密钥解密。非对称密钥加密算法的典型代表是**RSA**。</div>
<div>利用已经获得的信息情报来消除一个情报系统的不确定性就是解密。因此，密码学的最高境界就是无论敌方获取多少密文，也无法消除己方情报系统的不确定性。</div>
<div>

1.  找两个很大的素数P和Q，越大越好，然后计算它们的乘绩：N=P*Q;M=(P-1)*(Q-1)
2.  找一个和M互素的整数E(E和M最小公约数为1）
3.  找一个整数D，使E*D/M余1，即E*D mod M=1
4.  其中E是公钥，谁都可以用来加密，D是私钥用于解密，一定要保存好，乘积N是公开的，被人知道也无所谓
5.  对X加密得到Y----X^(E) mod N=Y
6.  根据费马小定理，用如下公式解密 <wbr /> Y^(D) mod N=X
</div>
<div></div>
<div>**最大熵（Maximum Entropy）模型**：需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。此时概率分布的信息熵最大。（就是要保留全部的不确定性，将风险降到最小。）</div>
<div>**通用迭代算法GIS（Generalized Iterative Scaling）：**最原始的最大熵模型训练方法，是一个典型的**期望值最大化算法（EM）**</div>
<div>

1.  假定第零次迭代的初始模型为等概率的均匀分布。
2.  用第N次迭代的模型来估算每种信息特征在训练数据中的分布。如果超过了实际的，就把相应的模型参数变小。否者，将它们变大。
3.  重复步骤2直至收敛。
<div>**IIS**：改进迭代算法，使得最大熵模型的训练时间缩短了一到两个数量级。</div>
</div>
<div></div>
<div>[布隆过滤器](https://zh.wikipedia.org/zh/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8)：一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。</div>
<div></div>
<div>**[贝叶斯网络（信念网络）](https://zh.wikipedia.org/wiki/%E8%B2%9D%E6%B0%8F%E7%B6%B2%E8%B7%AF)**：一个加权的有向图，是马尔可夫链的扩展。马尔可夫链是贝叶斯网络的特例。</div>
<div>得到贝叶斯网络拓扑结果和各个状态之间相关概率的过程分别称为结构训练和参数训练，统称训练。</div>
<div>从理论上讲，贝叶斯网络的训练是一个 **NP-Complete **问题。</div>
<div></div>
<div>[条件随机场](http://baike.baidu.com/view/1394699.htm)：如同马尔可夫随机场，条件随机场为具有无向的图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场中，随机变量 Y 的分布为条件机率，给定的观察值则为随机变量 X。</div>
<div></div>
<div>[维特比算法](http://zh.wikipedia.org/zh/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95)：针对篱笆网络的有向图的最短路径问题。</div>
<div></div>
<div>**频分多址（FDMA）**：对频率进行切分，每一路通信使用一个不同的频率，对讲机采用的就是这个原理。</div>
<div>**时分多址（TDMA）**：将同一频带按时间分成很多份。</div>
<div>**码分多址（CDMA）**：扩频传输。每个发送者有自己不同的密码，接收者在接收到不通信号时，通过密码过滤掉自己无法解码的信号。根据不同的密码区分发送，因此称为码多分址。</div>
<div></div>
<div>**[期望最大值算法](http://zh.wikipedia.org/zh/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95)（Expectation Maximization Alogorithm）**</div>
<div>**E过程**：根据现有的模型，计算各个观测数据输入到模型中的计算结果，称为期望值计算过程。</div>
<div>**M过程**：重新计算模型模型参数，以最大化期望值。</div>
<div></div>
<div>[**逻辑回归模型**](http://www.cnblogs.com/zgw21cn/archive/2008/08/29/1279634.html)</div>
